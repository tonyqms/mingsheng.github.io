<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Designing KPIs for Genomics Operations - Mingsheng Qi</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        :root {
            --bg-dark: #0a0e27;
            --bg-card: #1a1f3a;
            --accent-blue: #00d4ff;
            --accent-green: #00ff88;
            --accent-purple: #b24bf3;
            --text-primary: #ffffff;
            --text-secondary: #a0a6c9;
        }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
            background: var(--bg-dark);
            color: var(--text-primary);
            line-height: 1.8;
        }

        nav {
            position: fixed;
            top: 0;
            width: 100%;
            background: rgba(10, 14, 39, 0.95);
            backdrop-filter: blur(10px);
            padding: 1.5rem 5%;
            z-index: 1000;
            border-bottom: 1px solid rgba(0, 212, 255, 0.1);
        }

        nav ul {
            list-style: none;
            display: flex;
            justify-content: center;
            gap: 3rem;
            flex-wrap: wrap;
        }

        nav a {
            color: var(--text-secondary);
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s;
        }

        nav a:hover {
            color: var(--accent-blue);
        }

        .back-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.8rem 1.5rem;
            background: var(--bg-card);
            border: 1px solid var(--accent-blue);
            border-radius: 8px;
            color: var(--accent-blue);
            text-decoration: none;
            font-weight: 600;
            transition: all 0.3s;
            margin-bottom: 2rem;
        }

        .back-btn:hover {
            background: var(--accent-blue);
            color: var(--bg-dark);
            transform: translateX(-5px);
        }

        article {
            max-width: 800px;
            margin: 0 auto;
            padding: 8rem 5% 5rem;
        }

        .article-header {
            margin-bottom: 3rem;
        }

        .article-category {
            display: inline-block;
            padding: 0.5rem 1rem;
            background: linear-gradient(135deg, var(--accent-purple), var(--accent-blue));
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 600;
            margin-bottom: 1.5rem;
        }

        h1 {
            font-size: clamp(2rem, 5vw, 3rem);
            line-height: 1.2;
            margin-bottom: 1rem;
            background: linear-gradient(135deg, var(--accent-blue), var(--accent-green));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .article-meta {
            color: var(--text-secondary);
            font-size: 0.95rem;
            display: flex;
            gap: 2rem;
            flex-wrap: wrap;
            margin-bottom: 2rem;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .article-excerpt {
            font-size: 1.2rem;
            color: var(--text-secondary);
            font-style: italic;
            padding-left: 1.5rem;
            border-left: 4px solid var(--accent-blue);
            margin-bottom: 3rem;
        }

        .article-content {
            color: var(--text-secondary);
            font-size: 1.1rem;
        }

        .article-content h2 {
            color: var(--text-primary);
            font-size: 2rem;
            margin-top: 3rem;
            margin-bottom: 1.5rem;
            position: relative;
            padding-bottom: 0.5rem;
        }

        .article-content h2::after {
            content: '';
            position: absolute;
            bottom: 0;
            left: 0;
            width: 60px;
            height: 3px;
            background: linear-gradient(90deg, var(--accent-blue), var(--accent-green));
        }

        .article-content h3 {
            color: var(--accent-blue);
            font-size: 1.5rem;
            margin-top: 2rem;
            margin-bottom: 1rem;
        }

        .article-content p {
            margin-bottom: 1.5rem;
        }

        .article-content ul, .article-content ol {
            margin-bottom: 1.5rem;
            padding-left: 2rem;
        }

        .article-content li {
            margin-bottom: 0.8rem;
        }

        .article-content blockquote {
            padding: 1.5rem;
            background: var(--bg-card);
            border-left: 4px solid var(--accent-green);
            border-radius: 8px;
            margin: 2rem 0;
            font-style: italic;
        }

        .highlight-box {
            background: var(--bg-card);
            padding: 2rem;
            border-radius: 12px;
            border: 1px solid var(--accent-blue);
            margin: 2rem 0;
        }

        .share-section {
            margin-top: 4rem;
            padding-top: 3rem;
            border-top: 1px solid rgba(0, 212, 255, 0.2);
        }

        .share-section h3 {
            color: var(--text-primary);
            margin-bottom: 1rem;
        }

        .share-buttons {
            display: flex;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .share-btn {
            padding: 0.8rem 1.5rem;
            background: var(--bg-card);
            border: 1px solid var(--accent-blue);
            border-radius: 8px;
            color: var(--accent-blue);
            text-decoration: none;
            font-weight: 600;
            transition: all 0.3s;
        }

        .share-btn:hover {
            background: var(--accent-blue);
            color: var(--bg-dark);
            transform: translateY(-3px);
        }

        .related-posts {
            margin-top: 4rem;
            padding: 3rem;
            background: var(--bg-card);
            border-radius: 15px;
            border: 1px solid rgba(0, 212, 255, 0.1);
        }

        .related-posts h3 {
            color: var(--text-primary);
            margin-bottom: 2rem;
            font-size: 1.8rem;
        }

        .related-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 1.5rem;
        }

        .related-card {
            padding: 1.5rem;
            background: var(--bg-dark);
            border-radius: 10px;
            border: 1px solid rgba(0, 212, 255, 0.1);
            text-decoration: none;
            color: inherit;
            transition: all 0.3s;
        }

        .related-card:hover {
            border-color: var(--accent-green);
            transform: translateY(-5px);
        }

        .related-card h4 {
            color: var(--accent-green);
            margin-bottom: 0.5rem;
            font-size: 1.1rem;
        }

        .related-card p {
            color: var(--text-secondary);
            font-size: 0.9rem;
        }

        footer {
            text-align: center;
            padding: 3rem 5%;
            border-top: 1px solid rgba(0, 212, 255, 0.1);
            color: var(--text-secondary);
            margin-top: 5rem;
        }

        @media (max-width: 768px) {
            nav ul {
                gap: 1.5rem;
            }

            .article-meta {
                flex-direction: column;
                gap: 0.5rem;
            }
        }
    </style>
</head>
<body>
    <nav>
        <ul>
            <li><a href="./#home">Home</a></li>
            <li><a href="./#about">About</a></li>
            <li><a href="./#blog">Blog</a></li>
            <li><a href="./#portfolio">Portfolio</a></li>
            <li><a href="./#contact">Contact</a></li>
        </ul>
    </nav>

    <article>
        <a href="./#blog" class="back-btn">‚Üê Back to Blog</a>

        <header class="article-header">
            <span class="article-category">Technical Deep-Dive</span>
            <h1>Designing KPIs for Genomics Operations</h1>
            
            <div class="article-meta">
                <span class="meta-item">üìÖ June 12, 2025</span>
                <span class="meta-item">‚è±Ô∏è 13 min read</span>
                <span class="meta-item">‚úçÔ∏è Mingsheng Qi, Ph.D.</span>
            </div>

            <p class="article-excerpt">
                Beyond "samples processed": metrics that actually predict bottlenecks, optimize resource utilization, and keep pipelines running smoothly.
            </p>
        </header>

        <div class="article-content">
            <p>
                "How's the pipeline doing?" It's the question every genomics operations manager hears weekly. And too often, the answer is vague: "Pretty good" or "We're a bit behind" or "It depends."
            </p>

            <p>
                The problem isn't lack of data‚Äîgenomics generates data by the terabyte. The problem is lack of the <em>right</em> metrics. Most labs track inputs (samples received) and outputs (data delivered) but have no visibility into what happens in between. When problems arise, they react instead of predict.
            </p>

            <p>
                After managing end-to-end sequencing and bioinformatics operations for multiple organizations, I've learned that good KPIs (Key Performance Indicators) are the difference between flying blind and flying instruments. Here's how to design metrics that actually help you run a tight operation.
            </p>

            <h2>The Problem with Vanity Metrics</h2>

            <p>
                Most genomics KPIs fall into the "vanity metrics" trap‚Äînumbers that look impressive but don't drive decisions.
            </p>

            <h3>Vanity Metric Examples:</h3>

            <ul>
                <li><strong>"We processed 5,000 samples this quarter!"</strong> Sounds great. But at what cost? What was the error rate? Did half of them fail QC?</li>
                <li><strong>"We generated 50 TB of sequence data!"</strong> Okay, but was it the right data? Did it answer the biological questions?</li>
                <li><strong>"Our sequencers ran at 95% uptime!"</strong> Impressive, unless they were running low-priority samples while urgent projects sat in the queue.</li>
            </ul>

            <blockquote>
                "If a metric doesn't change your behavior or decisions, it's not a KPI‚Äîit's just a number."
            </blockquote>

            <h2>The KPI Framework: Four Categories</h2>

            <p>
                Effective genomics KPIs fall into four categories, each serving a different operational purpose:
            </p>

            <h3>1. Throughput Metrics: Are we delivering?</h3>

            <p>
                These measure output and productivity. They answer: "How much are we getting done?"
            </p>

            <h3>2. Quality Metrics: Are we delivering well?</h3>

            <p>
                These measure accuracy and reliability. They answer: "Is our output usable?"
            </p>

            <h3>3. Efficiency Metrics: Are we delivering smartly?</h3>

            <p>
                These measure resource utilization and cost-effectiveness. They answer: "Are we wasting resources?"
            </p>

            <h3>4. Timeliness Metrics: Are we delivering on time?</h3>

            <p>
                These measure speed and predictability. They answer: "Can stakeholders plan around us?"
            </p>

            <p>
                A balanced KPI dashboard includes metrics from all four categories. Optimize only one, and you'll create problems elsewhere (e.g., maximize throughput at the expense of quality).
            </p>

            <h2>Throughput Metrics That Matter</h2>

            <h3>Samples Processed (with context)</h3>

            <p>
                Don't just count samples‚Äîstratify by type and complexity.
            </p>

            <div class="highlight-box">
                <p><strong>Example breakdown:</strong></p>
                <ul>
                    <li>Simple WGS: 200 samples</li>
                    <li>Complex library prep (target enrichment): 50 samples</li>
                    <li>Multi-omic (RNA + WGS): 30 samples</li>
                </ul>
                <p>Total: 280 samples, but not all samples are equal effort.</p>
            </div>

            <h3>Pipeline Throughput (Weighted)</h3>

            <p>
                Assign complexity weights to different assay types. A 30X WGS sample might be "1 unit," while a custom amplicon panel with specialized QC is "3 units."
            </p>

            <p>
                Track: <strong>Total units delivered per week/month</strong>
            </p>

            <p>
                This gives you a normalized throughput metric that accounts for project complexity.
            </p>

            <h3>Sequencer Utilization</h3>

            <p>
                Don't just track uptime‚Äîtrack <strong>productive utilization</strong>: percentage of instrument time spent generating billable data (vs. wasted runs, QC tests, or idle time).
            </p>

            <p>
                Formula: <code>(Billable flowcells / Total flowcells run) √ó 100%</code>
            </p>

            <p>
                Target: 75-85%. Below 75% means you're underutilizing expensive capital. Above 85% means you have no buffer for urgent projects.
            </p>

            <h2>Quality Metrics That Predict Problems</h2>

            <h3>First-Pass Success Rate</h3>

            <p>
                Percentage of samples that pass QC on the first attempt. This is your most important quality metric.
            </p>

            <p>
                Formula: <code>(Samples passing QC / Total samples submitted) √ó 100%</code>
            </p>

            <p>
                Target: >90% for routine assays. If you're consistently below 85%, you have a systematic problem‚Äîbad sample prep, poor SOPs, or inadequate front-end QC.
            </p>

            <h3>Re-Run Rate</h3>

            <p>
                Percentage of samples requiring sequencing re-runs due to low depth, failed libraries, or other technical issues.
            </p>

            <p>
                This metric directly impacts cost and timeline. Every re-run consumes reagents, instrument time, and analyst bandwidth.
            </p>

            <p>
                Target: <5% for mature assays.
            </p>

            <h3>Data Quality Distributions</h3>

            <p>
                Don't just track averages‚Äîtrack distributions. For example:
            </p>

            <ul>
                <li><strong>Mapping rate distribution:</strong> Is your median 95% but your 10th percentile 60%? You have outlier samples dragging down quality.</li>
                <li><strong>Coverage uniformity:</strong> Are you consistently hitting target depth, or is it highly variable?</li>
                <li><strong>Contamination rates:</strong> Track across batches to catch systematic issues.</li>
            </ul>

            <p>
                I built automated QC dashboards (Streamlit + Plotly) that visualize these distributions in real-time. Spotting a batch effect at run time saves weeks of troubleshooting later.
            </p>

            <h2>Efficiency Metrics: Doing More with Less</h2>

            <h3>Cost per Sample (Fully Loaded)</h3>

            <p>
                Track not just reagent costs, but <strong>fully loaded costs</strong>:
            </p>

            <ul>
                <li>Reagents and consumables</li>
                <li>Labor (prep + analysis)</li>
                <li>Compute resources (cloud costs)</li>
                <li>Instrument depreciation</li>
                <li>Overhead (facility, admin)</li>
            </ul>

            <p>
                This gives you a true cost per sample, which you can benchmark against external providers or use for internal pricing.
            </p>

            <h3>Compute Efficiency</h3>

            <p>
                For cloud-based pipelines, track:
            </p>

            <ul>
                <li><strong>Cost per sample analyzed:</strong> Are you over-provisioning instances?</li>
                <li><strong>Idle compute time:</strong> Are you spinning up VMs that sit unused?</li>
                <li><strong>Storage costs:</strong> Are you hoarding intermediate files you'll never need?</li>
            </ul>

            <p>
                We cut our AWS costs by 40% just by implementing better job scheduling and automatic cleanup of intermediate files.
            </p>

            <h3>Analyst Time per Project</h3>

            <p>
                How many analyst-hours go into a typical project? Track separately for:
            </p>

            <ul>
                <li>Pipeline setup and QC</li>
                <li>Troubleshooting and re-runs</li>
                <li>Custom analysis</li>
                <li>Stakeholder communication</li>
            </ul>

            <p>
                If your analysts spend 50% of their time troubleshooting, you need better automation or more robust workflows.
            </p>

            <h2>Timeliness Metrics: Predictability is Power</h2>

            <h3>On-Time Delivery Rate</h3>

            <p>
                Percentage of projects delivered by the agreed SLA date.
            </p>

            <p>
                Target: >90%. Below 80% means you're consistently overpromising or have unaddressed bottlenecks.
            </p>

            <h3>Cycle Time by Stage</h3>

            <p>
                Break down total cycle time into stages:
            </p>

            <ul>
                <li>Sample intake to library prep: X days</li>
                <li>Library prep to sequencing: Y days</li>
                <li>Sequencing to analysis complete: Z days</li>
            </ul>

            <p>
                This identifies bottlenecks. If 80% of your delay is in one stage, that's where you optimize.
            </p>

            <h3>Queue Depth and Wait Time</h3>

            <p>
                How many projects are in queue? What's the average wait from submission to start?
            </p>

            <p>
                This is your leading indicator of capacity issues. When queue depth grows steadily, you're undersized for demand.
            </p>

            <h2>Advanced KPIs: Leading vs. Lagging Indicators</h2>

            <p>
                Most genomics metrics are <strong>lagging indicators</strong>‚Äîthey tell you what happened (samples processed, projects delivered). Useful for reporting, but not for prevention.
            </p>

            <p>
                <strong>Leading indicators</strong> predict problems before they hit. Examples:
            </p>

            <h3>Sample QC Failure Trends</h3>

            <p>
                If your sample pass rate drops from 95% to 88% over two weeks, you have a problem brewing‚Äîmaybe a reagent lot issue, a protocol drift, or a collaborator submitting poor samples.
            </p>

            <p>
                Catch it early (leading indicator), and you can intervene. Catch it late (lagging indicator), and you've wasted weeks.
            </p>

            <h3>Compute Resource Utilization Spikes</h3>

            <p>
                If your cloud costs suddenly jump 30%, it might mean:
            </p>

            <ul>
                <li>A runaway job consuming resources</li>
                <li>Inefficient pipeline code deployed</li>
                <li>Someone accidentally left VMs running</li>
            </ul>

            <p>
                Alerting on this in real-time (leading) is much better than discovering it at month-end (lagging).
            </p>

            <h3>Stakeholder Satisfaction (NPS)</h3>

            <p>
                Survey clients/collaborators quarterly: "How likely are you to recommend our services?" (0-10 scale)
            </p>

            <p>
                Declining NPS is a leading indicator that something's wrong operationally‚Äîeven if your throughput metrics look fine.
            </p>

            <h2>Building Your KPI Dashboard</h2>

            <p>
                Metrics are useless if no one looks at them. You need a dashboard that makes KPIs visible and actionable.
            </p>

            <h3>Tool Stack</h3>

            <p>
                At Solis, we built our dashboard with:
            </p>

            <ul>
                <li><strong>Python Streamlit:</strong> For interactive web app</li>
                <li><strong>Plotly:</strong> For visualizations</li>
                <li><strong>PostgreSQL:</strong> For storing pipeline metadata</li>
                <li><strong>Automated data collection:</strong> Scripts that pull metrics from LIMS, pipeline logs, and cloud APIs</li>
            </ul>

            <p>
                Update frequency: Daily for operational metrics, weekly for strategic metrics.
            </p>

            <h3>Dashboard Design Principles</h3>

            <div class="highlight-box">
                <h4>Effective Dashboard Features:</h4>
                <ol>
                    <li><strong>At-a-glance status:</strong> Red/yellow/green indicators for key metrics</li>
                    <li><strong>Drill-down capability:</strong> Click a metric to see underlying data</li>
                    <li><strong>Historical trends:</strong> Show last 3 months so you can spot patterns</li>
                    <li><strong>Contextual targets:</strong> Always show target alongside actual (e.g., "On-time rate: 87% [Target: 90%]")</li>
                    <li><strong>Automated alerts:</strong> Email/Slack when metrics fall below threshold</li>
                </ol>
            </div>

            <h3>Who Sees What?</h3>

            <p>
                Different stakeholders need different views:
            </p>

            <ul>
                <li><strong>Operations team:</strong> Full dashboard, daily updates, all leading indicators</li>
                <li><strong>Leadership:</strong> High-level summary (throughput, cost, on-time rate), weekly</li>
                <li><strong>Clients:</strong> Project-specific status (samples in queue, ETA, QC results), on-demand</li>
            </ul>

            <h2>Common KPI Mistakes and How to Avoid Them</h2>

            <h3>Mistake 1: Too Many Metrics</h3>

            <p>
                More metrics ‚â† better insights. If you're tracking 40 KPIs, you're tracking zero KPIs (because no one can process that much info).
            </p>

            <p>
                <strong>Fix:</strong> Limit to 10-12 core KPIs. Everything else is supporting data you review periodically, not daily.
            </p>

            <h3>Mistake 2: Metrics Without Targets</h3>

            <p>
                "Our on-time rate is 85%." Okay... is that good? Bad? Improving?
            </p>

            <p>
                <strong>Fix:</strong> Every KPI needs a target. Targets can be:
            </p>

            <ul>
                <li>Industry benchmarks (if available)</li>
                <li>Historical performance (e.g., "maintain 90% or better")</li>
                <li>Aspirational goals (e.g., "improve from 75% to 85% by Q4")</li>
            </ul>

            <h3>Mistake 3: Gaming the Metrics</h3>

            <p>
                When you tie KPIs to performance reviews or bonuses, people optimize for the metric rather than the outcome.
            </p>

            <p>
                Example: If you reward "samples processed," people will prioritize easy samples over complex ones‚Äîeven if the complex ones are more valuable.
            </p>

            <p>
                <strong>Fix:</strong> Use balanced scorecards. Optimize for multiple metrics simultaneously so gaming one doesn't work.
            </p>

            <h3>Mistake 4: Set-and-Forget Metrics</h3>

            <p>
                Your operation evolves. Your KPIs should too.
            </p>

            <p>
                <strong>Fix:</strong> Review your KPI framework quarterly. Ask:
            </p>

            <ul>
                <li>Are we still tracking the right things?</li>
                <li>Have any metrics become irrelevant?</li>
                <li>Are there new bottlenecks we should monitor?</li>
            </ul>

            <h2>Case Study: Using KPIs to Solve a Real Problem</h2>

            <p>
                At Solis, we noticed our on-time delivery rate dropped from 92% to 78% over two months. Our vanity metrics (samples processed, data generated) looked fine. What was wrong?
            </p>

            <h3>Step 1: Drill Down into Cycle Time</h3>

            <p>
                We broke down cycle time by stage and found the bottleneck: analysis turnaround had doubled from 3 days to 6 days.
            </p>

            <h3>Step 2: Investigate the Bottleneck</h3>

            <p>
                Why? We checked:
            </p>

            <ul>
                <li>Compute costs (normal)</li>
                <li>Pipeline failures (no increase)</li>
                <li>Analyst workload (bingo!)</li>
            </ul>

            <p>
                One analyst was on leave, and the remaining team was overwhelmed with custom analysis requests.
            </p>

            <h3>Step 3: Intervene</h3>

            <p>
                We temporarily paused Tier 3 (custom) projects, focused on clearing Tier 1 backlog, and hired a contractor for 8 weeks.
            </p>

            <h3>Step 4: Monitor Recovery</h3>

            <p>
                Within 3 weeks, on-time rate was back to 88%. Within 6 weeks, 93%‚Äîbetter than before.
            </p>

            <p>
                <strong>Lesson:</strong> Without KPIs, we would have noticed the problem much later (angry clients, missed deadlines). With KPIs, we caught it early and fixed it fast.
            </p>

            <h2>The KPIs I Actually Track</h2>

            <p>
                Here's my personal KPI dashboard for genomics operations (simplified for clarity):
            </p>

            <div class="highlight-box">
                <h4>Core KPIs (Reviewed Daily):</h4>
                <ol>
                    <li><strong>Active projects:</strong> Number in queue vs. capacity</li>
                    <li><strong>On-time delivery rate:</strong> Rolling 30-day average</li>
                    <li><strong>First-pass QC success rate:</strong> By assay type</li>
                    <li><strong>Sequencer utilization:</strong> Productive vs. idle time</li>
                    <li><strong>Compute costs:</strong> Daily spend vs. budget</li>
                </ol>

                <h4>Secondary KPIs (Reviewed Weekly):</h4>
                <ol start="6">
                    <li><strong>Re-run rate:</strong> By failure cause (library, sequencing, analysis)</li>
                    <li><strong>Cycle time by stage:</strong> Identify bottlenecks</li>
                    <li><strong>Cost per sample:</strong> By assay type</li>
                    <li><strong>Client NPS:</strong> Quarterly surveys</li>
                </ol>

                <h4>Strategic KPIs (Reviewed Monthly):</h4>
                <ol start="10">
                    <li><strong>Revenue per FTE:</strong> Efficiency benchmark</li>
                    <li><strong>Capacity utilization:</strong> Are we over/under capacity?</li>
                    <li><strong>Repeat client rate:</strong> Are clients coming back?</li>
                </ol>
            </div>

            <h2>Implementing KPIs: Start Small, Iterate</h2>

            <p>
                Don't try to build the perfect KPI system on day one. Start with:
            </p>

            <ol>
                <li><strong>Pick 5 core metrics</strong> that address your biggest pain points.</li>
                <li><strong>Manually track them for a month</strong> to validate they're useful.</li>
                <li><strong>Automate collection</strong> once you've proven value.</li>
                <li><strong>Build the dashboard</strong> to make metrics visible.</li>
                <li><strong>Iterate:</strong> Add/remove metrics based on what's actually driving decisions.</li>
            </ol>

            <h2>Final Thoughts: Metrics as Culture</h2>

            <p>
                The best genomics operations teams I've seen aren't necessarily the ones with the fanciest instruments or biggest budgets‚Äîthey're the ones that operate data-driven.
            </p>

            <p>
                They know their numbers. They spot trends before they become crises. They optimize based on evidence, not hunches.
            </p>

            <p>
                And critically: they use metrics to empower their team, not punish them. KPIs should illuminate problems so you can solve them together, not create blame.
            </p>

            <blockquote>
                "You can't improve what you don't measure. But measuring the wrong things makes you worse, not better."
            </blockquote>

            <p>
                Choose your KPIs wisely. Track them consistently. Act on them decisively. That's how you run a world-class genomics operation.
            </p>
        </div>

        <div class="share-section">
            <h3>Share this post</h3>
            <div class="share-buttons">
                <a href="#" class="share-btn">üê¶ Twitter</a>
                <a href="#" class="share-btn">üíº LinkedIn</a>
                <a href="#" class="share-btn">üìß Email</a>
            </div>
        </div>

        <div class="related-posts">
            <h3>Related Posts</h3>
            <div class="related-grid">
                <a href="./blog-reproducible-pipelines.html" class="related-card">
                    <h4>The ROI of Reproducible Pipelines</h4>
                    <p>Building systems that track metrics automatically.</p>
                </a>
                <a href="./blog-scientific-services.html" class="related-card">
                    <h4>Building Scientific Services That Scale</h4>
                    <p>How SLAs and KPIs work together.</p>
                </a>
                <a href="./blog-skim-seq.html" class="related-card">
                    <h4>Skim-Seq + Imputation</h4>
                    <p>Optimizing cost-per-sample metrics.</p>
                </a>
            </div>
        </div>
    </article>

    <footer>
        <p>&copy; 2026 Mingsheng Qi, Ph.D. | <a href="./" style="color: var(--accent-blue); text-decoration: none;">Return to Home</a></p>
    </footer>
</body>
</html>